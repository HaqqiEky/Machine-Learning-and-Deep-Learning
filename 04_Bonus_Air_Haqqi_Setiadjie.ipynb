{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset MNIST"
      ],
      "metadata": {
        "id": "kMN3k-buXAvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TLIX0_ZW6eR",
        "outputId": "243651da-c521-4242-e9e4-4d2ac9a0e428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 146241958.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 68593258.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46482450.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16422869.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desain Pemilihan Konfigurasi Model"
      ],
      "metadata": {
        "id": "VF7QzjjTXmCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Baseline Model\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Wide Model\n",
        "class WideModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WideModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256) # Lebih lebar\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Deep Model\n",
        "class DeepModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128) # Lebih dalam\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iV7zK71mXoxz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looping untuk training model"
      ],
      "metadata": {
        "id": "HnzylU1YXuaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')"
      ],
      "metadata": {
        "id": "N9KPNdOEYHje"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Model"
      ],
      "metadata": {
        "id": "kiAQ2nHfYJTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "6-I6vX8obtHy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    baseline_model = BaselineModel()\n",
        "    wide_model = WideModel()\n",
        "    deep_model = DeepModel()\n",
        "\n",
        "    # Pelatihan Model\n",
        "    num_epochs = 15\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
        "    optimizer_wide = optim.Adam(wide_model.parameters(), lr=0.001)\n",
        "    optimizer_deep = optim.Adam(deep_model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Baseline Model: \")\n",
        "    train_model(baseline_model, train_loader, criterion, optimizer_baseline, num_epochs)\n",
        "    print(\"\")\n",
        "\n",
        "    print(\"Wide Model: \")\n",
        "    train_model(wide_model, train_loader, criterion, optimizer_wide, num_epochs)\n",
        "    print(\"\")\n",
        "\n",
        "    print(\"Deep Model: \")\n",
        "    train_model(deep_model, train_loader, criterion, optimizer_deep, num_epochs)\n",
        "    print(\"\")\n",
        "\n",
        "    # Evaluasi Model\n",
        "    accuracy_baseline = evaluate_model(baseline_model, test_loader)\n",
        "    accuracy_wide = evaluate_model(wide_model, test_loader)\n",
        "    accuracy_deep = evaluate_model(deep_model, test_loader)\n",
        "\n",
        "    # Penjelasan Hasil\n",
        "    print(f'Accuracy - Baseline Model: {accuracy_baseline}')\n",
        "    print(f'Accuracy - Wide Model: {accuracy_wide}')\n",
        "    print(f'Accuracy - Deep Model: {accuracy_deep}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5U-usZN7dhh",
        "outputId": "afb5463d-6e69-49a0-87fa-f68348330297"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model: \n",
            "Epoch 1, Loss: 0.39421698010202916\n",
            "Epoch 2, Loss: 0.19271088528957195\n",
            "Epoch 3, Loss: 0.14113070277660006\n",
            "Epoch 4, Loss: 0.1138913159314265\n",
            "Epoch 5, Loss: 0.09561898981802848\n",
            "Epoch 6, Loss: 0.08416499210489807\n",
            "Epoch 7, Loss: 0.07255322549135676\n",
            "Epoch 8, Loss: 0.06611793020418458\n",
            "Epoch 9, Loss: 0.05855294445063919\n",
            "Epoch 10, Loss: 0.057163893432566906\n",
            "Epoch 11, Loss: 0.05184060289213724\n",
            "Epoch 12, Loss: 0.04816057155854992\n",
            "Epoch 13, Loss: 0.04346180899853834\n",
            "Epoch 14, Loss: 0.0429977639825262\n",
            "Epoch 15, Loss: 0.035814029516123266\n",
            "\n",
            "Wide Model: \n",
            "Epoch 1, Loss: 0.3435114584029166\n",
            "Epoch 2, Loss: 0.15207365417837113\n",
            "Epoch 3, Loss: 0.11164240651070945\n",
            "Epoch 4, Loss: 0.0886775097901077\n",
            "Epoch 5, Loss: 0.07835332361305121\n",
            "Epoch 6, Loss: 0.06728839244780892\n",
            "Epoch 7, Loss: 0.05987028676039998\n",
            "Epoch 8, Loss: 0.0531685647312929\n",
            "Epoch 9, Loss: 0.04760867730577363\n",
            "Epoch 10, Loss: 0.042161036980525056\n",
            "Epoch 11, Loss: 0.039758235319648616\n",
            "Epoch 12, Loss: 0.035149737024433546\n",
            "Epoch 13, Loss: 0.034726421995023175\n",
            "Epoch 14, Loss: 0.029114236653267248\n",
            "Epoch 15, Loss: 0.030429712559373528\n",
            "\n",
            "Deep Model: \n",
            "Epoch 1, Loss: 0.37378869976586243\n",
            "Epoch 2, Loss: 0.16624527899667557\n",
            "Epoch 3, Loss: 0.12623391505569093\n",
            "Epoch 4, Loss: 0.10361680424939007\n",
            "Epoch 5, Loss: 0.08765860449330115\n",
            "Epoch 6, Loss: 0.07961985330754665\n",
            "Epoch 7, Loss: 0.06820106298698962\n",
            "Epoch 8, Loss: 0.06199970687957985\n",
            "Epoch 9, Loss: 0.056653462672682564\n",
            "Epoch 10, Loss: 0.05042829367696796\n",
            "Epoch 11, Loss: 0.04798917673197565\n",
            "Epoch 12, Loss: 0.04526924992868576\n",
            "Epoch 13, Loss: 0.041499862177503496\n",
            "Epoch 14, Loss: 0.03622230605335473\n",
            "Epoch 15, Loss: 0.035322860026531525\n",
            "\n",
            "Accuracy - Baseline Model: 0.974\n",
            "Accuracy - Wide Model: 0.9719\n",
            "Accuracy - Deep Model: 0.9752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Penjelasan performa dari setiap model"
      ],
      "metadata": {
        "id": "yOAZFIt23tN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model"
      ],
      "metadata": {
        "id": "5jjnj0ADEnmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model ini adalah model dasar dengan 3 lapisan tersembunyi yang terdiri dari 128 neuron serta meiliki arsitektur 128 -> 64 -> 10, ini mengindikasikan bahwa model memiliki kemampuan yang cukup baik untuk mempelajari pola dalam data, dan pelatihan konvergen pada tingkat loss yang rendah secara signifikan. Akurasi yang dicapai adalah sekitar 97,4%, yang menunjukkan model dapat mengklasifikasikan digit-digit MNIST dengan baik."
      ],
      "metadata": {
        "id": "F8vnlEI6EuWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wide Model"
      ],
      "metadata": {
        "id": "b1q1tEGaFHs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model ini lebih lebar dibandingkan dengan Baseline model, dengan jumlah neuron yang lebih banyak sebesar 256 neuron dan 3 lapisan tersembunyi serta memiliki arsitektur  256 -> 128 -> 10. Terlihat bahwa model wide mencapai loss yang lebih rendah lebih cepat selama epoch pertama dibandingkan dengan Baseline model. Model ini mencapai akurasi 97.19%. Perbedaan ini menunjukkan bahwa tambahan neuron dapat membantu percepatan konvergensi."
      ],
      "metadata": {
        "id": "golTyZ3DFNtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Model"
      ],
      "metadata": {
        "id": "TPaDPO3kF0Mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model ini lebih dalam dibandingkan dengan Baseline model, dengan empat lapisan tersembunyi dan jumlah neuron sebesar 128 neuron serta memiliki arsitektur 128 -> 128 -> 128 -> 10.  Loss model deep menurun secara signifikan selama epoch kedua, menunjukkan kemampuan model untuk memahami pola yang lebih kompleks dalam data, namun pada beberapa epoch berikutnya, penurunan loss menjadi lebih lambat dibandingkan dengan model lainnya. Akurasi yang dicapai oleh model deep adalah sekitar 96.52. Ini menunjukkan bahwa kedalaman model dapat membantu dalam pemahaman yang lebih baik terhadap data, meskipun pelatihan memerlukan beberapa epoch lebih lama."
      ],
      "metadata": {
        "id": "SFV2EveFF96s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan"
      ],
      "metadata": {
        "id": "jJm2MIIwHT_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Model tetap memiliki performa yang baik, terutama mengingat kesederhanaan arsitekturnya, sedangkan Model wide dan deep masing-masing memiliki kelebihan dan kekurangan. Model wide cenderung konvergen lebih cepat dengan akurasi yang baik, sementara model deep memiliki kemampuan untuk memahami pola yang lebih kompleks dalam data, meskipun memerlukan lebih banyak epoch untuk konvergensi dengan waktu yang lebih lama."
      ],
      "metadata": {
        "id": "RfLkt4-mHYod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulannya, pemilihan arsitektur model tergantung pada kebutuhan dan sumber daya yang tersedia. Model wide cocok untuk aplikasi yang memerlukan pelatihan cepat, sedangkan model deep cocok untuk tugas yang memerlukan pemahaman yang lebih mendalam terhadap data. Jadi, dari interpretasi di atas, Model Deep adalah pilihan terbaik karena memiliki akurasi tertinggi dan kinerja yang baik secara keseluruhan dalam mengklasifikasikan data MNIST."
      ],
      "metadata": {
        "id": "xj5tG-bKH0DI"
      }
    }
  ]
}